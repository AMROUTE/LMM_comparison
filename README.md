ä¸»è¦æ˜¯é’ˆå¯¹ChatGLM3-6bä¸åƒé—®ä¸¤ä¸ªå¤§æ¨¡å‹ä¹‹é—´çš„éƒ¨ç½²ä»¥åŠè¯­è¨€ç”Ÿæˆå¯¹æ¯”
âœ… 1. ChatGLM3-6B ç¯å¢ƒé…ç½®

ğŸ“¦ æ¨èç¯å¢ƒ
	â€¢	Pythonï¼š3.10
	â€¢	PyTorchï¼š>=1.13.1ï¼ˆå»ºè®® 2.x ä»¥ä¸Šï¼‰
	â€¢	transformersï¼š>=4.32.0
	â€¢	å¿…éœ€ä¾èµ–ï¼špip install torch transformers accelerate sentencepiece
ğŸ“ æ¨¡å‹æ–‡ä»¶
ä» HuggingFace ä¸‹è½½æ¨¡å‹ THUDM/chatglm3-6b
å¦‚æœæ— æ³•è”ç½‘ï¼Œå¯æ‰‹åŠ¨ä¸‹è½½åæ”¾åˆ° /mnt/data/chatglm3-6b
âœ… ç¤ºä¾‹ä»£ç 
//from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("/mnt/data/chatglm3-6b", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("/mnt/data/chatglm3-6b", trust_remote_code=True).eval()

history = []
response, history = model.chat(tokenizer, "ä½ å¥½", history=history)
print(response)//
âœ… 2. Qwen-7B Chat ç¯å¢ƒé…ç½®

ğŸ“¦ æ¨èç¯å¢ƒ
	â€¢	Pythonï¼š3.10
	â€¢	PyTorchï¼š>=2.0
	â€¢	transformersï¼š>=4.35.0
	â€¢	éœ€è¦å®‰è£…ï¼špip install torch transformers accelerate einops tiktoken
ğŸ“ æ¨¡å‹æ–‡ä»¶
 HuggingFace ä¸‹è½½ï¼‰ï¼šQwen/Qwen-7B-Chat
è‹¥æ‰‹åŠ¨ä¸‹è½½ï¼šä¿å­˜åˆ° /mnt/data/Qwen-7B-Chat

âœ… ç¤ºä¾‹ä»£ç 
//from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/mnt/data/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("/mnt/data/Qwen-7B-Chat", trust_remote_code=True).eval()

response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)//
